{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook merges all the different data together, that we have acquried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import seaborn as sns\n",
    "\n",
    "from src.sentiment import sentiment_vader\n",
    "from src.imdb_api.imdbscraper import ImdbScraper\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trailers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We loop through our file list, which consist of the 4 networks. \n",
    "The column 'network' is created in the dataframe, and the given network is added to said column.\n",
    "At the end of each loop, we add the data to the list 'dfs'\n",
    "\n",
    "The dataframe 'trailers' is then created, using 'dfs' and the columns defined in 'cols'\n",
    "\n",
    "'''\n",
    "file_list = ['hbo', 'amazon', 'netflix', 'disney']\n",
    "dfs = []\n",
    "for file in file_list:\n",
    "    df = pd.read_csv('../../data/interim/trailers/' + file + '.csv')\n",
    "    df['network'] = file\n",
    "    dfs.append(df)\n",
    "\n",
    "cols = ['channelId', 'network', 'videoId', 'videoTitle', 'publishTime']\n",
    "trailers = pd.concat(dfs)[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell follows the structure as the trailer cell above.\n",
    "'''\n",
    "\n",
    "file_list = ['hbo_comments', 'amazon_comments', 'netflix_comments', 'disney_comments']\n",
    "dfs = []\n",
    "for file in file_list:\n",
    "    df = pd.read_csv('../../data/raw/comments/' + file + '.csv')\n",
    "    dfs.append(df)\n",
    "\n",
    "comments = pd.concat(dfs)\n",
    "cols = ['videoId', 'commentId', 'textOriginal', 'likeCount', 'publishedAt']\n",
    "comments = comments[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping imdb and youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell follows the structure as the trailer & comments cells above.\n",
    "'''\n",
    "file_list = ['hbo', 'amazon', 'netflix', 'disney']\n",
    "dfs = []\n",
    "for file in file_list:\n",
    "    df = pd.read_csv('../../data/interim/match/' + file + '_match.csv', delimiter=';')\n",
    "    dfs.append(df)\n",
    "\n",
    "match = pd.concat(dfs).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marcu\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3418: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The data used here, is from an external source: datasets downloaded from the IMDb webiste. https://www.imdb.com/interfaces/\n",
    "We get the movie/show titles from 'title.basics.tsv' and the ratings from 'title.ratings.tsv'. \n",
    "--> They are joined on 'tconst' (IMDb's ID for a show/movie)\n",
    "'''\n",
    "\n",
    "imdb = pd.read_csv('../../data/external/imdb/title.basics.tsv', delimiter='\\t')\n",
    "ratings = pd.read_csv('../../data/external/imdb/title.ratings.tsv', delimiter='\\t')\n",
    "imdb = imdb.merge(ratings, on='tconst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb release dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scraper: Initializes the ImdbScraper() class. \n",
    "It scrapes the release dates, based on on 'tconst' it is given.\n",
    "A timeout of 1 second is implemented, to not get locked out from their website. \n",
    "'''\n",
    "\n",
    "# scraper = ImdbScraper()\n",
    "# scraper.scrape_dates_alternate(match.tconst, verbose=True, timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating a dataframe from the sraped data.\n",
    "The column 'releaseDateUS' is created from the scraped data and converted to a datetime object.\n",
    "'''\n",
    "\n",
    "# release_dates = pd.DataFrame(scraper.data)\n",
    "# release_dates['releaseDateUS'] = pd.to_datetime(release_dates.release_date_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release_dates.to_csv('release_dates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_dates = pd.read_csv('../../data/interim/release_dates.csv')\n",
    "cols = ['id', 'release_date_us']\n",
    "release_dates = release_dates[cols]\n",
    "release_dates = release_dates.rename(columns={'id':'tconst', 'release_date_us': 'releaseDateUS'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare annotations\n",
    "annot = pd.read_csv('../../data/interim/annotated.csv')\n",
    "annot = annot[['commentId', 'sentiment', 'annotator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_agg = annot.groupby('commentId').agg(\n",
    "    sentimentLabel=('sentiment', stats.mode)\n",
    ").reset_index()\n",
    "\n",
    "annot_agg.sentimentLabel = annot_agg.sentimentLabel.str[0].str[0]\n",
    "\n",
    "sentiment_map = {'Positive': 1, 'Neutral': 0, 'Negative': -1}\n",
    "annot_agg['sentimentScore'] = annot_agg.sentimentLabel.map(sentiment_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReturnYoutTubeDislikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare Return YouTube Dislikes\n",
    "ryd = pd.read_csv('../../data/raw/returnyoutubedislikes.csv')\n",
    "ryd = ryd[['videoId', 'likes', 'dislikes', 'viewCount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The big join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all dataframes together\n",
    "df = trailers.merge(comments, on='videoId')\n",
    "df = df.merge(ryd, on='videoId', how='left')\n",
    "df = df.merge(match, on='videoId')\n",
    "df = df.merge(imdb, on='tconst')\n",
    "df = df.merge(release_dates, on='tconst', how='left')\n",
    "df = df.merge(annot_agg, on='commentId', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate comment date offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    " # add an comment date offset column\n",
    "dt = pd.to_datetime(df.publishedAt)\n",
    "comment_date = dt.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['releaseDateUS'] = pd.to_datetime(df.releaseDateUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['commentDateOffset'] = (pd.to_datetime(comment_date) - df.releaseDateUS)\n",
    "df['commentDateOffset'] = df.commentDateOffset.astype('timedelta64[D]').astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../data/processed/dataset_no_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentimentPredictedRaw'] = df.textOriginal.astype(str).apply(sentiment_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = {'Positive': 1, 'Neutral': 0, 'Negative': -1}\n",
    "df['sentimentPredictedScore'] = df.sentimentPredictedRaw.str[-1].map(sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
